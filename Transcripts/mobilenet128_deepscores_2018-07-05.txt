**********************
Windows PowerShell transcript start
Start time: 20180706024429
Username: DONKEY\Alex
RunAs User: DONKEY\Alex
Configuration Name: 
Machine: DONKEY (Microsoft Windows NT 10.0.16299.0)
Host Application: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -Command if((Get-ExecutionPolicy ) -ne 'AllSigned') { Set-ExecutionPolicy -Scope Process Bypass }; & 'C:\Users\Alex\Repositories\keras-retinanet\train.ps1'
Process ID: 14372
PSVersion: 5.1.16299.492
PSEdition: Desktop
PSCompatibleVersions: 1.0, 2.0, 3.0, 4.0, 5.0, 5.1.16299.492
BuildVersion: 10.0.16299.492
CLRVersion: 4.0.30319.42000
WSManStackVersion: 3.0
PSRemotingProtocolVersion: 2.3
SerializationVersion: 1.1.0.1
**********************
Transcript started, output file is C:/Users/Alex/Repositories/keras-retinanet/Transcripts/mobilenet128_deepscores_2018-07-05.txt
C:\Programmieren\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.
In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
train.py:370: UserWarning: Using experimental backbone mobilenet128_1. Only resnet50 has been properly tested.
  'Using experimental backbone {}. Only resnet50 has been properly tested.'.format(parsed_args.backbone))
2018-07-06 02:44:31.536709: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not c
ompiled to use: AVX2
2018-07-06 02:44:31.749894: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.00GiB freeMemory: 9.10GiB
2018-07-06 02:44:31.760434: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1435] Adding visible gpu devices: 0
2018-07-06 02:44:32.345974: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-06 02:44:32.349735: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:929]      0
2018-07-06 02:44:32.352353: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:942] 0:   N
2018-07-06 02:44:32.357587: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/devic
e:GPU:0 with 8805 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Creating model, this may take a second...
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, None, None, 3 0           input_1[0][0]
__________________________________________________________________________________________________
conv1 (Conv2D)                  (None, None, None, 3 864         conv1_pad[0][0]
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, None, None, 3 128         conv1[0][0]
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, None, None, 3 0           conv1_bn[0][0]
__________________________________________________________________________________________________
conv_pad_1 (ZeroPadding2D)      (None, None, None, 3 0           conv1_relu[0][0]
__________________________________________________________________________________________________
conv_dw_1 (DepthwiseConv2D)     (None, None, None, 3 288         conv_pad_1[0][0]
__________________________________________________________________________________________________
conv_dw_1_bn (BatchNormalizatio (None, None, None, 3 128         conv_dw_1[0][0]
__________________________________________________________________________________________________
conv_dw_1_relu (Activation)     (None, None, None, 3 0           conv_dw_1_bn[0][0]
__________________________________________________________________________________________________
conv_pw_1 (Conv2D)              (None, None, None, 6 2048        conv_dw_1_relu[0][0]
__________________________________________________________________________________________________
conv_pw_1_bn (BatchNormalizatio (None, None, None, 6 256         conv_pw_1[0][0]
__________________________________________________________________________________________________
conv_pw_1_relu (Activation)     (None, None, None, 6 0           conv_pw_1_bn[0][0]
__________________________________________________________________________________________________
conv_pad_2 (ZeroPadding2D)      (None, None, None, 6 0           conv_pw_1_relu[0][0]
__________________________________________________________________________________________________
conv_dw_2 (DepthwiseConv2D)     (None, None, None, 6 576         conv_pad_2[0][0]
__________________________________________________________________________________________________
conv_dw_2_bn (BatchNormalizatio (None, None, None, 6 256         conv_dw_2[0][0]
__________________________________________________________________________________________________
conv_dw_2_relu (Activation)     (None, None, None, 6 0           conv_dw_2_bn[0][0]
__________________________________________________________________________________________________
conv_pw_2 (Conv2D)              (None, None, None, 1 8192        conv_dw_2_relu[0][0]
__________________________________________________________________________________________________
conv_pw_2_bn (BatchNormalizatio (None, None, None, 1 512         conv_pw_2[0][0]
__________________________________________________________________________________________________
conv_pw_2_relu (Activation)     (None, None, None, 1 0           conv_pw_2_bn[0][0]
__________________________________________________________________________________________________
conv_pad_3 (ZeroPadding2D)      (None, None, None, 1 0           conv_pw_2_relu[0][0]
__________________________________________________________________________________________________
conv_dw_3 (DepthwiseConv2D)     (None, None, None, 1 1152        conv_pad_3[0][0]
__________________________________________________________________________________________________
conv_dw_3_bn (BatchNormalizatio (None, None, None, 1 512         conv_dw_3[0][0]
__________________________________________________________________________________________________
conv_dw_3_relu (Activation)     (None, None, None, 1 0           conv_dw_3_bn[0][0]
__________________________________________________________________________________________________
conv_pw_3 (Conv2D)              (None, None, None, 1 16384       conv_dw_3_relu[0][0]
__________________________________________________________________________________________________
conv_pw_3_bn (BatchNormalizatio (None, None, None, 1 512         conv_pw_3[0][0]
__________________________________________________________________________________________________
conv_pw_3_relu (Activation)     (None, None, None, 1 0           conv_pw_3_bn[0][0]
__________________________________________________________________________________________________
conv_pad_4 (ZeroPadding2D)      (None, None, None, 1 0           conv_pw_3_relu[0][0]
__________________________________________________________________________________________________
conv_dw_4 (DepthwiseConv2D)     (None, None, None, 1 1152        conv_pad_4[0][0]
__________________________________________________________________________________________________
conv_dw_4_bn (BatchNormalizatio (None, None, None, 1 512         conv_dw_4[0][0]
__________________________________________________________________________________________________
conv_dw_4_relu (Activation)     (None, None, None, 1 0           conv_dw_4_bn[0][0]
__________________________________________________________________________________________________
conv_pw_4 (Conv2D)              (None, None, None, 2 32768       conv_dw_4_relu[0][0]
__________________________________________________________________________________________________
conv_pw_4_bn (BatchNormalizatio (None, None, None, 2 1024        conv_pw_4[0][0]
__________________________________________________________________________________________________
conv_pw_4_relu (Activation)     (None, None, None, 2 0           conv_pw_4_bn[0][0]
__________________________________________________________________________________________________
conv_pad_5 (ZeroPadding2D)      (None, None, None, 2 0           conv_pw_4_relu[0][0]
__________________________________________________________________________________________________
conv_dw_5 (DepthwiseConv2D)     (None, None, None, 2 2304        conv_pad_5[0][0]
__________________________________________________________________________________________________
conv_dw_5_bn (BatchNormalizatio (None, None, None, 2 1024        conv_dw_5[0][0]
__________________________________________________________________________________________________
conv_dw_5_relu (Activation)     (None, None, None, 2 0           conv_dw_5_bn[0][0]
__________________________________________________________________________________________________
conv_pw_5 (Conv2D)              (None, None, None, 2 65536       conv_dw_5_relu[0][0]
__________________________________________________________________________________________________
conv_pw_5_bn (BatchNormalizatio (None, None, None, 2 1024        conv_pw_5[0][0]
__________________________________________________________________________________________________
conv_pw_5_relu (Activation)     (None, None, None, 2 0           conv_pw_5_bn[0][0]
__________________________________________________________________________________________________
conv_pad_6 (ZeroPadding2D)      (None, None, None, 2 0           conv_pw_5_relu[0][0]
__________________________________________________________________________________________________
conv_dw_6 (DepthwiseConv2D)     (None, None, None, 2 2304        conv_pad_6[0][0]
__________________________________________________________________________________________________
conv_dw_6_bn (BatchNormalizatio (None, None, None, 2 1024        conv_dw_6[0][0]
__________________________________________________________________________________________________
conv_dw_6_relu (Activation)     (None, None, None, 2 0           conv_dw_6_bn[0][0]
__________________________________________________________________________________________________
conv_pw_6 (Conv2D)              (None, None, None, 5 131072      conv_dw_6_relu[0][0]
__________________________________________________________________________________________________
conv_pw_6_bn (BatchNormalizatio (None, None, None, 5 2048        conv_pw_6[0][0]
__________________________________________________________________________________________________
conv_pw_6_relu (Activation)     (None, None, None, 5 0           conv_pw_6_bn[0][0]
__________________________________________________________________________________________________
conv_pad_7 (ZeroPadding2D)      (None, None, None, 5 0           conv_pw_6_relu[0][0]
__________________________________________________________________________________________________
conv_dw_7 (DepthwiseConv2D)     (None, None, None, 5 4608        conv_pad_7[0][0]
__________________________________________________________________________________________________
conv_dw_7_bn (BatchNormalizatio (None, None, None, 5 2048        conv_dw_7[0][0]
__________________________________________________________________________________________________
conv_dw_7_relu (Activation)     (None, None, None, 5 0           conv_dw_7_bn[0][0]
__________________________________________________________________________________________________
conv_pw_7 (Conv2D)              (None, None, None, 5 262144      conv_dw_7_relu[0][0]
__________________________________________________________________________________________________
conv_pw_7_bn (BatchNormalizatio (None, None, None, 5 2048        conv_pw_7[0][0]
__________________________________________________________________________________________________
conv_pw_7_relu (Activation)     (None, None, None, 5 0           conv_pw_7_bn[0][0]
__________________________________________________________________________________________________
conv_pad_8 (ZeroPadding2D)      (None, None, None, 5 0           conv_pw_7_relu[0][0]
__________________________________________________________________________________________________
conv_dw_8 (DepthwiseConv2D)     (None, None, None, 5 4608        conv_pad_8[0][0]
__________________________________________________________________________________________________
conv_dw_8_bn (BatchNormalizatio (None, None, None, 5 2048        conv_dw_8[0][0]
__________________________________________________________________________________________________
conv_dw_8_relu (Activation)     (None, None, None, 5 0           conv_dw_8_bn[0][0]
__________________________________________________________________________________________________
conv_pw_8 (Conv2D)              (None, None, None, 5 262144      conv_dw_8_relu[0][0]
__________________________________________________________________________________________________
conv_pw_8_bn (BatchNormalizatio (None, None, None, 5 2048        conv_pw_8[0][0]
__________________________________________________________________________________________________
conv_pw_8_relu (Activation)     (None, None, None, 5 0           conv_pw_8_bn[0][0]
__________________________________________________________________________________________________
conv_pad_9 (ZeroPadding2D)      (None, None, None, 5 0           conv_pw_8_relu[0][0]
__________________________________________________________________________________________________
conv_dw_9 (DepthwiseConv2D)     (None, None, None, 5 4608        conv_pad_9[0][0]
__________________________________________________________________________________________________
conv_dw_9_bn (BatchNormalizatio (None, None, None, 5 2048        conv_dw_9[0][0]
__________________________________________________________________________________________________
conv_dw_9_relu (Activation)     (None, None, None, 5 0           conv_dw_9_bn[0][0]
__________________________________________________________________________________________________
conv_pw_9 (Conv2D)              (None, None, None, 5 262144      conv_dw_9_relu[0][0]
__________________________________________________________________________________________________
conv_pw_9_bn (BatchNormalizatio (None, None, None, 5 2048        conv_pw_9[0][0]
__________________________________________________________________________________________________
conv_pw_9_relu (Activation)     (None, None, None, 5 0           conv_pw_9_bn[0][0]
__________________________________________________________________________________________________
conv_pad_10 (ZeroPadding2D)     (None, None, None, 5 0           conv_pw_9_relu[0][0]
__________________________________________________________________________________________________
conv_dw_10 (DepthwiseConv2D)    (None, None, None, 5 4608        conv_pad_10[0][0]
__________________________________________________________________________________________________
conv_dw_10_bn (BatchNormalizati (None, None, None, 5 2048        conv_dw_10[0][0]
__________________________________________________________________________________________________
conv_dw_10_relu (Activation)    (None, None, None, 5 0           conv_dw_10_bn[0][0]
__________________________________________________________________________________________________
conv_pw_10 (Conv2D)             (None, None, None, 5 262144      conv_dw_10_relu[0][0]
__________________________________________________________________________________________________
conv_pw_10_bn (BatchNormalizati (None, None, None, 5 2048        conv_pw_10[0][0]
__________________________________________________________________________________________________
conv_pw_10_relu (Activation)    (None, None, None, 5 0           conv_pw_10_bn[0][0]
__________________________________________________________________________________________________
conv_pad_11 (ZeroPadding2D)     (None, None, None, 5 0           conv_pw_10_relu[0][0]
__________________________________________________________________________________________________
conv_dw_11 (DepthwiseConv2D)    (None, None, None, 5 4608        conv_pad_11[0][0]
__________________________________________________________________________________________________
conv_dw_11_bn (BatchNormalizati (None, None, None, 5 2048        conv_dw_11[0][0]
__________________________________________________________________________________________________
conv_dw_11_relu (Activation)    (None, None, None, 5 0           conv_dw_11_bn[0][0]
__________________________________________________________________________________________________
conv_pw_11 (Conv2D)             (None, None, None, 5 262144      conv_dw_11_relu[0][0]
__________________________________________________________________________________________________
conv_pw_11_bn (BatchNormalizati (None, None, None, 5 2048        conv_pw_11[0][0]
__________________________________________________________________________________________________
conv_pw_11_relu (Activation)    (None, None, None, 5 0           conv_pw_11_bn[0][0]
__________________________________________________________________________________________________
conv_pad_12 (ZeroPadding2D)     (None, None, None, 5 0           conv_pw_11_relu[0][0]
__________________________________________________________________________________________________
conv_dw_12 (DepthwiseConv2D)    (None, None, None, 5 4608        conv_pad_12[0][0]
__________________________________________________________________________________________________
conv_dw_12_bn (BatchNormalizati (None, None, None, 5 2048        conv_dw_12[0][0]
__________________________________________________________________________________________________
conv_dw_12_relu (Activation)    (None, None, None, 5 0           conv_dw_12_bn[0][0]
__________________________________________________________________________________________________
conv_pw_12 (Conv2D)             (None, None, None, 1 524288      conv_dw_12_relu[0][0]
__________________________________________________________________________________________________
conv_pw_12_bn (BatchNormalizati (None, None, None, 1 4096        conv_pw_12[0][0]
__________________________________________________________________________________________________
conv_pw_12_relu (Activation)    (None, None, None, 1 0           conv_pw_12_bn[0][0]
__________________________________________________________________________________________________
conv_pad_13 (ZeroPadding2D)     (None, None, None, 1 0           conv_pw_12_relu[0][0]
__________________________________________________________________________________________________
conv_dw_13 (DepthwiseConv2D)    (None, None, None, 1 9216        conv_pad_13[0][0]
__________________________________________________________________________________________________
conv_dw_13_bn (BatchNormalizati (None, None, None, 1 4096        conv_dw_13[0][0]
__________________________________________________________________________________________________
conv_dw_13_relu (Activation)    (None, None, None, 1 0           conv_dw_13_bn[0][0]
__________________________________________________________________________________________________
conv_pw_13 (Conv2D)             (None, None, None, 1 1048576     conv_dw_13_relu[0][0]
__________________________________________________________________________________________________
conv_pw_13_bn (BatchNormalizati (None, None, None, 1 4096        conv_pw_13[0][0]
__________________________________________________________________________________________________
conv_pw_13_relu (Activation)    (None, None, None, 1 0           conv_pw_13_bn[0][0]
__________________________________________________________________________________________________
C5_reduced (Conv2D)             (None, None, None, 2 262400      conv_pw_13_relu[0][0]
__________________________________________________________________________________________________
P5_upsampled (UpsampleLike)     (None, None, None, 2 0           C5_reduced[0][0]
                                                                 conv_pw_11_relu[0][0]
__________________________________________________________________________________________________
C4_reduced (Conv2D)             (None, None, None, 2 131328      conv_pw_11_relu[0][0]
__________________________________________________________________________________________________
P4_merged (Add)                 (None, None, None, 2 0           P5_upsampled[0][0]
                                                                 C4_reduced[0][0]
__________________________________________________________________________________________________
P4_upsampled (UpsampleLike)     (None, None, None, 2 0           P4_merged[0][0]
                                                                 conv_pw_5_relu[0][0]
__________________________________________________________________________________________________
C3_reduced (Conv2D)             (None, None, None, 2 65792       conv_pw_5_relu[0][0]
__________________________________________________________________________________________________
P6 (Conv2D)                     (None, None, None, 2 2359552     conv_pw_13_relu[0][0]
__________________________________________________________________________________________________
P3_merged (Add)                 (None, None, None, 2 0           P4_upsampled[0][0]
                                                                 C3_reduced[0][0]
__________________________________________________________________________________________________
C6_relu (Activation)            (None, None, None, 2 0           P6[0][0]
__________________________________________________________________________________________________
P3 (Conv2D)                     (None, None, None, 2 590080      P3_merged[0][0]
__________________________________________________________________________________________________
P4 (Conv2D)                     (None, None, None, 2 590080      P4_merged[0][0]
__________________________________________________________________________________________________
P5 (Conv2D)                     (None, None, None, 2 590080      C5_reduced[0][0]
__________________________________________________________________________________________________
P7 (Conv2D)                     (None, None, None, 2 590080      C6_relu[0][0]
__________________________________________________________________________________________________
regression_submodel (Model)     (None, None, 4)      2443300     P3[0][0]
                                                                 P4[0][0]
                                                                 P5[0][0]
                                                                 P6[0][0]
                                                                 P7[0][0]
__________________________________________________________________________________________________
classification_submodel (Model) (None, None, 40)     3190120     P3[0][0]
                                                                 P4[0][0]
                                                                 P5[0][0]
                                                                 P6[0][0]
                                                                 P7[0][0]
__________________________________________________________________________________________________
regression (Concatenate)        (None, None, 4)      0           regression_submodel[1][0]
                                                                 regression_submodel[2][0]
                                                                 regression_submodel[3][0]
                                                                 regression_submodel[4][0]
                                                                 regression_submodel[5][0]
__________________________________________________________________________________________________
classification (Concatenate)    (None, None, 40)     0           classification_submodel[1][0]
                                                                 classification_submodel[2][0]
                                                                 classification_submodel[3][0]
                                                                 classification_submodel[4][0]
                                                                 classification_submodel[5][0]
==================================================================================================
Total params: 14,041,676
Trainable params: 14,019,788
Non-trainable params: 21,888
__________________________________________________________________________________________________
None
C:\Programmieren\Anaconda3\lib\site-packages\keras\callbacks.py:999: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.
  warnings.warn('`epsilon` argument is deprecated and '
Epoch 1/500
Strides = [8, 16, 32, 64, 128]
Sizes = [32, 64, 128, 256, 512]
Ratios = [0.5 1.  2. ]
Scales = [1.         1.25992105 1.58740105]
Classifying samples as positive if they overlap with ground truth more than 0.5 and as background if they overlap less than 0.4. Overlaps inbetween will be ignored
2018-07-06 02:45:03.514762: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.16GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 02:45:03.665181: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.91GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 02:45:03.700442: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.90GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 02:45:03.826797: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.16GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 02:45:03.962239: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.62GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
60/60 [==============================] - 264s 4s/step - loss: 3.4024 - regression_loss: 2.3797 - classification_loss: 1.0228
mAP: 0.0131

Epoch 00001: mAP improved from -inf to 0.01313, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 2/500
60/60 [==============================] - 36s 593ms/step - loss: 2.1641 - regression_loss: 1.6732 - classification_loss: 0.4909
mAP: 0.0245

Epoch 00002: mAP improved from 0.01313 to 0.02454, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 3/500
60/60 [==============================] - 36s 596ms/step - loss: 1.5430 - regression_loss: 1.1321 - classification_loss: 0.4109
mAP: 0.0327

Epoch 00003: mAP improved from 0.02454 to 0.03267, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 4/500
60/60 [==============================] - 36s 596ms/step - loss: 1.2380 - regression_loss: 0.8808 - classification_loss: 0.3572
mAP: 0.0478

Epoch 00004: mAP improved from 0.03267 to 0.04783, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 5/500
60/60 [==============================] - 36s 596ms/step - loss: 1.0615 - regression_loss: 0.7650 - classification_loss: 0.2964
mAP: 0.0499

Epoch 00005: mAP improved from 0.04783 to 0.04995, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 6/500
60/60 [==============================] - 36s 596ms/step - loss: 0.8676 - regression_loss: 0.6252 - classification_loss: 0.2424
mAP: 0.0504

Epoch 00006: mAP improved from 0.04995 to 0.05040, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 7/500
60/60 [==============================] - 36s 597ms/step - loss: 0.7712 - regression_loss: 0.5586 - classification_loss: 0.2126
mAP: 0.0508

Epoch 00007: mAP improved from 0.05040 to 0.05078, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 8/500
60/60 [==============================] - 36s 596ms/step - loss: 0.7098 - regression_loss: 0.5204 - classification_loss: 0.1893
mAP: 0.0511

Epoch 00008: mAP improved from 0.05078 to 0.05109, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 9/500
60/60 [==============================] - 36s 598ms/step - loss: 0.7256 - regression_loss: 0.5678 - classification_loss: 0.1578
mAP: 0.0513

Epoch 00009: mAP improved from 0.05109 to 0.05132, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 10/500
60/60 [==============================] - 36s 596ms/step - loss: 0.6052 - regression_loss: 0.4752 - classification_loss: 0.1300
mAP: 0.0514

Epoch 00010: mAP improved from 0.05132 to 0.05136, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 11/500
60/60 [==============================] - 36s 595ms/step - loss: 0.6177 - regression_loss: 0.5021 - classification_loss: 0.1156
mAP: 0.0522

Epoch 00011: mAP improved from 0.05136 to 0.05224, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 12/500
60/60 [==============================] - 36s 595ms/step - loss: 0.5120 - regression_loss: 0.4211 - classification_loss: 0.0909
mAP: 0.0615

Epoch 00012: mAP improved from 0.05224 to 0.06150, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 13/500
60/60 [==============================] - 36s 595ms/step - loss: 0.5153 - regression_loss: 0.4378 - classification_loss: 0.0775
mAP: 0.0645

Epoch 00013: mAP improved from 0.06150 to 0.06453, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 14/500
60/60 [==============================] - 36s 597ms/step - loss: 0.4277 - regression_loss: 0.3540 - classification_loss: 0.0737
mAP: 0.0769

Epoch 00014: mAP improved from 0.06453 to 0.07694, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 15/500
60/60 [==============================] - 36s 595ms/step - loss: 0.3880 - regression_loss: 0.3298 - classification_loss: 0.0581
mAP: 0.0657

Epoch 00015: mAP did not improve from 0.07694
Epoch 16/500
60/60 [==============================] - 36s 596ms/step - loss: 0.3956 - regression_loss: 0.3430 - classification_loss: 0.0526
mAP: 0.0663

Epoch 00016: mAP did not improve from 0.07694
Epoch 17/500
60/60 [==============================] - 36s 595ms/step - loss: 0.3408 - regression_loss: 0.2931 - classification_loss: 0.0477
mAP: 0.0761

Epoch 00017: mAP did not improve from 0.07694
Epoch 18/500
60/60 [==============================] - 36s 596ms/step - loss: 0.3629 - regression_loss: 0.3183 - classification_loss: 0.0447
mAP: 0.0720

Epoch 00018: mAP did not improve from 0.07694
Epoch 19/500
60/60 [==============================] - 36s 594ms/step - loss: 0.3496 - regression_loss: 0.3125 - classification_loss: 0.0371
mAP: 0.0718

Epoch 00019: mAP did not improve from 0.07694
Epoch 20/500
60/60 [==============================] - 36s 596ms/step - loss: 0.3083 - regression_loss: 0.2668 - classification_loss: 0.0415
mAP: 0.0793

Epoch 00020: mAP improved from 0.07694 to 0.07934, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 21/500
60/60 [==============================] - 36s 595ms/step - loss: 0.2885 - regression_loss: 0.2576 - classification_loss: 0.0309
mAP: 0.0808

Epoch 00021: mAP improved from 0.07934 to 0.08079, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 22/500
60/60 [==============================] - 36s 595ms/step - loss: 0.2892 - regression_loss: 0.2556 - classification_loss: 0.0336
mAP: 0.0722

Epoch 00022: mAP did not improve from 0.08079
Epoch 23/500
60/60 [==============================] - 36s 594ms/step - loss: 0.2982 - regression_loss: 0.2641 - classification_loss: 0.0341
mAP: 0.0811

Epoch 00023: mAP improved from 0.08079 to 0.08109, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 24/500
60/60 [==============================] - 36s 597ms/step - loss: 0.2838 - regression_loss: 0.2591 - classification_loss: 0.0246
mAP: 0.0719

Epoch 00024: mAP did not improve from 0.08109
Epoch 25/500
60/60 [==============================] - 36s 597ms/step - loss: 0.2815 - regression_loss: 0.2536 - classification_loss: 0.0279
mAP: 0.0876

Epoch 00025: mAP improved from 0.08109 to 0.08758, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 26/500
60/60 [==============================] - 36s 595ms/step - loss: 0.2660 - regression_loss: 0.2335 - classification_loss: 0.0325
mAP: 0.0831

Epoch 00026: mAP did not improve from 0.08758
Epoch 27/500
60/60 [==============================] - 36s 596ms/step - loss: 0.2784 - regression_loss: 0.2577 - classification_loss: 0.0207
mAP: 0.0861

Epoch 00027: mAP did not improve from 0.08758
Epoch 28/500
60/60 [==============================] - 36s 594ms/step - loss: 0.2999 - regression_loss: 0.2774 - classification_loss: 0.0225
mAP: 0.0794

Epoch 00028: mAP did not improve from 0.08758
Epoch 29/500
60/60 [==============================] - 36s 596ms/step - loss: 0.2319 - regression_loss: 0.2152 - classification_loss: 0.0167
mAP: 0.0844

Epoch 00029: mAP did not improve from 0.08758
Epoch 30/500
60/60 [==============================] - 36s 595ms/step - loss: 0.2336 - regression_loss: 0.2192 - classification_loss: 0.0144
mAP: 0.0829

Epoch 00030: mAP did not improve from 0.08758
Epoch 31/500
60/60 [==============================] - 36s 595ms/step - loss: 0.1942 - regression_loss: 0.1805 - classification_loss: 0.0137
mAP: 0.0846

Epoch 00031: mAP did not improve from 0.08758
Epoch 32/500
60/60 [==============================] - 36s 596ms/step - loss: 0.2399 - regression_loss: 0.2273 - classification_loss: 0.0127
mAP: 0.0838

Epoch 00032: mAP did not improve from 0.08758
Epoch 33/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1903 - regression_loss: 0.1785 - classification_loss: 0.0117
mAP: 0.0834

Epoch 00033: mAP did not improve from 0.08758
Epoch 34/500
60/60 [==============================] - 36s 597ms/step - loss: 0.2041 - regression_loss: 0.1872 - classification_loss: 0.0169
mAP: 0.0813

Epoch 00034: mAP did not improve from 0.08758
Epoch 35/500
60/60 [==============================] - 36s 596ms/step - loss: 0.2256 - regression_loss: 0.2077 - classification_loss: 0.0179
mAP: 0.0879

Epoch 00035: mAP improved from 0.08758 to 0.08789, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 36/500
60/60 [==============================] - 36s 596ms/step - loss: 0.2127 - regression_loss: 0.2031 - classification_loss: 0.0095
mAP: 0.0747

Epoch 00036: mAP did not improve from 0.08789
Epoch 37/500
60/60 [==============================] - 36s 593ms/step - loss: 0.2022 - regression_loss: 0.1860 - classification_loss: 0.0162
mAP: 0.0835

Epoch 00037: mAP did not improve from 0.08789
Epoch 38/500
60/60 [==============================] - 36s 594ms/step - loss: 0.1906 - regression_loss: 0.1780 - classification_loss: 0.0126
mAP: 0.0814

Epoch 00038: mAP did not improve from 0.08789
Epoch 39/500
60/60 [==============================] - 36s 597ms/step - loss: 0.1658 - regression_loss: 0.1541 - classification_loss: 0.0116
mAP: 0.0834

Epoch 00039: mAP did not improve from 0.08789
Epoch 40/500
60/60 [==============================] - 36s 597ms/step - loss: 0.1463 - regression_loss: 0.1348 - classification_loss: 0.0115
mAP: 0.0892

Epoch 00040: mAP improved from 0.08789 to 0.08918, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 41/500
60/60 [==============================] - 36s 594ms/step - loss: 0.2071 - regression_loss: 0.2006 - classification_loss: 0.0065
mAP: 0.0839

Epoch 00041: mAP did not improve from 0.08918
Epoch 42/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1739 - regression_loss: 0.1673 - classification_loss: 0.0066
mAP: 0.0881

Epoch 00042: mAP did not improve from 0.08918
Epoch 43/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1296 - regression_loss: 0.1263 - classification_loss: 0.0033
mAP: 0.0850

Epoch 00043: mAP did not improve from 0.08918
Epoch 44/500
60/60 [==============================] - 36s 595ms/step - loss: 0.1547 - regression_loss: 0.1511 - classification_loss: 0.0036
mAP: 0.0897

Epoch 00044: mAP improved from 0.08918 to 0.08975, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 45/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1618 - regression_loss: 0.1584 - classification_loss: 0.0035
mAP: 0.0868

Epoch 00045: mAP did not improve from 0.08975
Epoch 46/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1864 - regression_loss: 0.1824 - classification_loss: 0.0040
mAP: 0.0887

Epoch 00046: mAP did not improve from 0.08975
Epoch 47/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1496 - regression_loss: 0.1440 - classification_loss: 0.0056
mAP: 0.0857

Epoch 00047: mAP did not improve from 0.08975
Epoch 48/500
60/60 [==============================] - 36s 595ms/step - loss: 0.1302 - regression_loss: 0.1173 - classification_loss: 0.0129
mAP: 0.0901

Epoch 00048: mAP improved from 0.08975 to 0.09006, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 49/500
60/60 [==============================] - 36s 595ms/step - loss: 0.1713 - regression_loss: 0.1617 - classification_loss: 0.0096
mAP: 0.0762

Epoch 00049: mAP did not improve from 0.09006
Epoch 50/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1571 - regression_loss: 0.1468 - classification_loss: 0.0103
mAP: 0.0893

Epoch 00050: mAP did not improve from 0.09006
Epoch 51/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1759 - regression_loss: 0.1710 - classification_loss: 0.0049
mAP: 0.0808

Epoch 00051: mAP did not improve from 0.09006
Epoch 52/500
60/60 [==============================] - 36s 597ms/step - loss: 0.1565 - regression_loss: 0.1520 - classification_loss: 0.0045
mAP: 0.0878

Epoch 00052: mAP did not improve from 0.09006
Epoch 53/500
60/60 [==============================] - 36s 595ms/step - loss: 0.1425 - regression_loss: 0.1363 - classification_loss: 0.0062
mAP: 0.0914

Epoch 00053: mAP improved from 0.09006 to 0.09139, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 54/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1446 - regression_loss: 0.1307 - classification_loss: 0.0139
mAP: 0.0914

Epoch 00054: mAP improved from 0.09139 to 0.09145, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 55/500
60/60 [==============================] - 36s 595ms/step - loss: 0.1440 - regression_loss: 0.1264 - classification_loss: 0.0177
mAP: 0.0885

Epoch 00055: mAP did not improve from 0.09145
Epoch 56/500
60/60 [==============================] - 36s 595ms/step - loss: 0.1388 - regression_loss: 0.1287 - classification_loss: 0.0101
mAP: 0.0845

Epoch 00056: mAP did not improve from 0.09145
Epoch 57/500
60/60 [==============================] - 36s 594ms/step - loss: 0.1148 - regression_loss: 0.1111 - classification_loss: 0.0037
mAP: 0.0916

Epoch 00057: mAP improved from 0.09145 to 0.09156, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 58/500
60/60 [==============================] - 36s 595ms/step - loss: 0.1482 - regression_loss: 0.1460 - classification_loss: 0.0022
mAP: 0.0891

Epoch 00058: mAP did not improve from 0.09156
Epoch 59/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1168 - regression_loss: 0.1150 - classification_loss: 0.0017
mAP: 0.0864

Epoch 00059: mAP did not improve from 0.09156
Epoch 60/500
60/60 [==============================] - 36s 593ms/step - loss: 0.0952 - regression_loss: 0.0941 - classification_loss: 0.0011
mAP: 0.0882

Epoch 00060: mAP did not improve from 0.09156
Epoch 61/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1058 - regression_loss: 0.1045 - classification_loss: 0.0013
mAP: 0.0924

Epoch 00061: mAP improved from 0.09156 to 0.09242, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 62/500
60/60 [==============================] - 36s 593ms/step - loss: 0.1378 - regression_loss: 0.1363 - classification_loss: 0.0015
mAP: 0.0851

Epoch 00062: mAP did not improve from 0.09242
Epoch 63/500
60/60 [==============================] - 36s 595ms/step - loss: 0.0992 - regression_loss: 0.0981 - classification_loss: 0.0011
mAP: 0.0857

Epoch 00063: mAP did not improve from 0.09242
Epoch 64/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1277 - regression_loss: 0.1261 - classification_loss: 0.0016
mAP: 0.0886

Epoch 00064: mAP did not improve from 0.09242
Epoch 65/500
60/60 [==============================] - 36s 595ms/step - loss: 0.0967 - regression_loss: 0.0953 - classification_loss: 0.0014
mAP: 0.0847

Epoch 00065: mAP did not improve from 0.09242
Epoch 66/500
60/60 [==============================] - 36s 594ms/step - loss: 0.1037 - regression_loss: 0.1029 - classification_loss: 8.5169e-04
mAP: 0.0892

Epoch 00066: mAP did not improve from 0.09242
Epoch 67/500
60/60 [==============================] - 36s 595ms/step - loss: 0.1136 - regression_loss: 0.1130 - classification_loss: 6.1711e-04
mAP: 0.0935

Epoch 00067: mAP improved from 0.09242 to 0.09350, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 68/500
60/60 [==============================] - 36s 598ms/step - loss: 0.1098 - regression_loss: 0.1089 - classification_loss: 8.9917e-04
mAP: 0.0962

Epoch 00068: mAP improved from 0.09350 to 0.09618, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 69/500
60/60 [==============================] - 36s 596ms/step - loss: 0.0900 - regression_loss: 0.0893 - classification_loss: 6.8031e-04
mAP: 0.0966

Epoch 00069: mAP improved from 0.09618 to 0.09661, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 70/500
60/60 [==============================] - 36s 597ms/step - loss: 0.0931 - regression_loss: 0.0923 - classification_loss: 7.2243e-04
mAP: 0.0855

Epoch 00070: mAP did not improve from 0.09661
Epoch 71/500
60/60 [==============================] - 36s 596ms/step - loss: 0.0934 - regression_loss: 0.0930 - classification_loss: 4.4407e-04
mAP: 0.0871

Epoch 00071: mAP did not improve from 0.09661
Epoch 72/500
60/60 [==============================] - 36s 595ms/step - loss: 0.0859 - regression_loss: 0.0855 - classification_loss: 4.0439e-04
mAP: 0.0902

Epoch 00072: mAP did not improve from 0.09661
Epoch 73/500
60/60 [==============================] - 36s 598ms/step - loss: 0.0921 - regression_loss: 0.0915 - classification_loss: 5.6475e-04
mAP: 0.0897

Epoch 00073: mAP did not improve from 0.09661
Epoch 74/500
60/60 [==============================] - 36s 594ms/step - loss: 0.0883 - regression_loss: 0.0853 - classification_loss: 0.0029
mAP: 0.1030

Epoch 00074: mAP improved from 0.09661 to 0.10297, saving model to ./snapshots\mobilenet128_1_mob_csv_2018-07-06_02-44.h5
Epoch 75/500
60/60 [==============================] - 36s 598ms/step - loss: 0.0947 - regression_loss: 0.0898 - classification_loss: 0.0049
mAP: 0.0879

Epoch 00075: mAP did not improve from 0.10297
Epoch 76/500
60/60 [==============================] - 36s 595ms/step - loss: 0.1382 - regression_loss: 0.1292 - classification_loss: 0.0090
mAP: 0.0955

Epoch 00076: mAP did not improve from 0.10297
Epoch 77/500
60/60 [==============================] - 36s 596ms/step - loss: 0.1167 - regression_loss: 0.1003 - classification_loss: 0.0163
mAP: 0.0873

Epoch 00077: mAP did not improve from 0.10297
Epoch 78/500
60/60 [==============================] - 36s 597ms/step - loss: 0.1141 - regression_loss: 0.0920 - classification_loss: 0.0221
mAP: 0.0891

Epoch 00078: mAP did not improve from 0.10297
Epoch 79/500
60/60 [==============================] - 36s 596ms/step - loss: 0.0951 - regression_loss: 0.0839 - classification_loss: 0.0112
mAP: 0.0789

Epoch 00079: mAP did not improve from 0.10297
Epoch 80/500
60/60 [==============================] - 36s 593ms/step - loss: 0.1114 - regression_loss: 0.0962 - classification_loss: 0.0152
mAP: 0.0888

Epoch 00080: mAP did not improve from 0.10297
Epoch 81/500
60/60 [==============================] - 36s 594ms/step - loss: 0.1252 - regression_loss: 0.1220 - classification_loss: 0.0032
mAP: 0.0898

Epoch 00081: mAP did not improve from 0.10297
Epoch 82/500
60/60 [==============================] - 36s 594ms/step - loss: 0.1035 - regression_loss: 0.1025 - classification_loss: 9.5131e-04
mAP: 0.0894

Epoch 00082: mAP did not improve from 0.10297
Epoch 83/500
60/60 [==============================] - 36s 593ms/step - loss: 0.0914 - regression_loss: 0.0907 - classification_loss: 6.8026e-04
mAP: 0.0837

Epoch 00083: mAP did not improve from 0.10297
Epoch 84/500
60/60 [==============================] - 36s 593ms/step - loss: 0.0925 - regression_loss: 0.0920 - classification_loss: 4.9989e-04
mAP: 0.0955

Epoch 00084: mAP did not improve from 0.10297
Epoch 85/500
60/60 [==============================] - 36s 594ms/step - loss: 0.0853 - regression_loss: 0.0849 - classification_loss: 4.0399e-04
mAP: 0.0867

Epoch 00085: mAP did not improve from 0.10297
Epoch 86/500
60/60 [==============================] - 36s 594ms/step - loss: 0.1111 - regression_loss: 0.1107 - classification_loss: 4.0026e-04
mAP: 0.0894

Epoch 00086: mAP did not improve from 0.10297
Epoch 87/500
60/60 [==============================] - 36s 594ms/step - loss: 0.1195 - regression_loss: 0.1191 - classification_loss: 4.0150e-04
mAP: 0.0878

Epoch 00087: mAP did not improve from 0.10297
Epoch 88/500
60/60 [==============================] - 35s 591ms/step - loss: 0.0856 - regression_loss: 0.0853 - classification_loss: 3.0316e-04
mAP: 0.0860

Epoch 00088: mAP did not improve from 0.10297
Epoch 89/500
60/60 [==============================] - 36s 594ms/step - loss: 0.0777 - regression_loss: 0.0775 - classification_loss: 2.2426e-04
mAP: 0.0890

Epoch 00089: mAP did not improve from 0.10297
Epoch 90/500
60/60 [==============================] - 36s 594ms/step - loss: 0.0597 - regression_loss: 0.0595 - classification_loss: 2.4768e-04
mAP: 0.0890

Epoch 00090: mAP did not improve from 0.10297

Epoch 00090: ReduceLROnPlateau reducing learning rate to 7.999999797903001e-05.
Epoch 91/500
60/60 [==============================] - 36s 594ms/step - loss: 0.0644 - regression_loss: 0.0642 - classification_loss: 2.1303e-04
mAP: 0.0864

Epoch 00091: mAP did not improve from 0.10297
Epoch 92/500
60/60 [==============================] - 36s 596ms/step - loss: 0.0608 - regression_loss: 0.0606 - classification_loss: 1.8191e-04
mAP: 0.0853

Epoch 00092: mAP did not improve from 0.10297
Epoch 93/500
60/60 [==============================] - 36s 595ms/step - loss: 0.0524 - regression_loss: 0.0521 - classification_loss: 2.1337e-04
mAP: 0.0879

Epoch 00093: mAP did not improve from 0.10297
Epoch 94/500
60/60 [==============================] - 36s 594ms/step - loss: 0.0567 - regression_loss: 0.0566 - classification_loss: 1.3316e-04
mAP: 0.0895

Epoch 00094: mAP did not improve from 0.10297
Epoch 95/500
60/60 [==============================] - 36s 595ms/step - loss: 0.0535 - regression_loss: 0.0533 - classification_loss: 1.3289e-04
mAP: 0.0867

Epoch 00095: mAP did not improve from 0.10297
Epoch 96/500
60/60 [==============================] - 36s 597ms/step - loss: 0.0513 - regression_loss: 0.0512 - classification_loss: 1.3361e-04
mAP: 0.0885

Epoch 00096: mAP did not improve from 0.10297
Epoch 97/500
60/60 [==============================] - 36s 595ms/step - loss: 0.0559 - regression_loss: 0.0558 - classification_loss: 1.3145e-04
mAP: 0.0913

Epoch 00097: mAP did not improve from 0.10297
Epoch 98/500
60/60 [==============================] - 36s 594ms/step - loss: 0.0465 - regression_loss: 0.0464 - classification_loss: 1.2810e-04
mAP: 0.0890

Epoch 00098: mAP did not improve from 0.10297
Epoch 99/500
60/60 [==============================] - 36s 595ms/step - loss: 0.0659 - regression_loss: 0.0658 - classification_loss: 1.3122e-04
mAP: 0.0887

Epoch 00099: mAP did not improve from 0.10297
Epoch 100/500
60/60 [==============================] - 36s 596ms/step - loss: 0.0595 - regression_loss: 0.0594 - classification_loss: 9.5452e-05
mAP: 0.0915

Epoch 00100: mAP did not improve from 0.10297
Epoch 101/500
60/60 [==============================] - 36s 596ms/step - loss: 0.0612 - regression_loss: 0.0611 - classification_loss: 9.6857e-05
mAP: 0.0899

Epoch 00101: mAP did not improve from 0.10297
Epoch 102/500
60/60 [==============================] - 36s 596ms/step - loss: 0.0614 - regression_loss: 0.0613 - classification_loss: 8.6079e-05
mAP: 0.0903

Epoch 00102: mAP did not improve from 0.10297
Epoch 103/500
60/60 [==============================] - 36s 595ms/step - loss: 0.0572 - regression_loss: 0.0571 - classification_loss: 1.1027e-04
mAP: 0.0871

Epoch 00103: mAP did not improve from 0.10297
Epoch 104/500
60/60 [==============================] - 36s 592ms/step - loss: 0.0485 - regression_loss: 0.0484 - classification_loss: 9.2270e-05
mAP: 0.0874

Epoch 00104: mAP did not improve from 0.10297
Epoch 105/500
60/60 [==============================] - 36s 595ms/step - loss: 0.0552 - regression_loss: 0.0551 - classification_loss: 9.1163e-05
mAP: 0.0918

Epoch 00105: mAP did not improve from 0.10297
Epoch 106/500
60/60 [==============================] - 36s 596ms/step - loss: 0.0616 - regression_loss: 0.0615 - classification_loss: 7.2756e-05
mAP: 0.0886

Epoch 00106: mAP did not improve from 0.10297

Epoch 00106: ReduceLROnPlateau reducing learning rate to 6.399999838322402e-05.
Epoch 107/500
60/60 [==============================] - 36s 596ms/step - loss: 0.0408 - regression_loss: 0.0408 - classification_loss: 7.1586e-05
mAP: 0.0937

Epoch 00107: mAP did not improve from 0.10297
Epoch 108/500
60/60 [==============================] - 36s 598ms/step - loss: 0.0289 - regression_loss: 0.0289 - classification_loss: 5.9192e-05
mAP: 0.0916

Epoch 00108: mAP did not improve from 0.10297
Epoch 109/500
60/60 [==============================] - 36s 597ms/step - loss: 0.0309 - regression_loss: 0.0308 - classification_loss: 6.0699e-05
mAP: 0.0956

Epoch 00109: mAP did not improve from 0.10297
Epoch 110/500
60/60 [==============================] - 36s 595ms/step - loss: 0.0341 - regression_loss: 0.0341 - classification_loss: 5.4025e-05
mAP: 0.0955

Epoch 00110: mAP did not improve from 0.10297
Epoch 111/500
60/60 [==============================] - 36s 594ms/step - loss: 0.0478 - regression_loss: 0.0478 - classification_loss: 5.3674e-05
mAP: 0.0875

Epoch 00111: mAP did not improve from 0.10297
Epoch 112/500
60/60 [==============================] - 36s 594ms/step - loss: 0.0318 - regression_loss: 0.0317 - classification_loss: 5.9322e-05
mAP: 0.0897

Epoch 00112: mAP did not improve from 0.10297
Epoch 113/500
60/60 [==============================] - 36s 595ms/step - loss: 0.0359 - regression_loss: 0.0358 - classification_loss: 5.0561e-05
mAP: 0.0908

Epoch 00113: mAP did not improve from 0.10297
Epoch 114/500
60/60 [==============================] - 36s 593ms/step - loss: 0.0451 - regression_loss: 0.0451 - classification_loss: 4.5810e-05
mAP: 0.0932

Epoch 00114: mAP did not improve from 0.10297
Epoch 00114: early stopping
Trained for: 1:21:53.863902
**********************
Windows PowerShell transcript end
End time: 20180706040636
**********************
