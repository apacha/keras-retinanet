**********************
Windows PowerShell transcript start
Start time: 20180706044911
Username: DONKEY\Alex
RunAs User: DONKEY\Alex
Configuration Name: 
Machine: DONKEY (Microsoft Windows NT 10.0.16299.0)
Host Application: C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -Command if((Get-ExecutionPolicy ) -ne 'AllSigned') { Set-ExecutionPolicy -Scope Process Bypass }; & 'C:\Users\Alex\Repositories\keras-retinanet\train.ps1'
Process ID: 14372
PSVersion: 5.1.16299.492
PSEdition: Desktop
PSCompatibleVersions: 1.0, 2.0, 3.0, 4.0, 5.0, 5.1.16299.492
BuildVersion: 10.0.16299.492
CLRVersion: 4.0.30319.42000
WSManStackVersion: 3.0
PSRemotingProtocolVersion: 2.3
SerializationVersion: 1.1.0.1
**********************
Transcript started, output file is C:/Users/Alex/Repositories/keras-retinanet/Transcripts/densenet121_muscima_2018-07-05.txt
__________________________________________________________________________________________________
conv5_block14_2_conv (Conv2D)   (None, None, None, 3 36864       conv5_block14_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block14_concat (Concatena (None, None, None, 9 0           conv5_block13_concat[0][0]
                                                                 conv5_block14_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block15_0_bn (BatchNormal (None, None, None, 9 3840        conv5_block14_concat[0][0]
__________________________________________________________________________________________________
conv5_block15_0_relu (Activatio (None, None, None, 9 0           conv5_block15_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block15_1_conv (Conv2D)   (None, None, None, 1 122880      conv5_block15_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block15_1_bn (BatchNormal (None, None, None, 1 512         conv5_block15_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block15_1_relu (Activatio (None, None, None, 1 0           conv5_block15_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block15_2_conv (Conv2D)   (None, None, None, 3 36864       conv5_block15_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block15_concat (Concatena (None, None, None, 9 0           conv5_block14_concat[0][0]
                                                                 conv5_block15_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block16_0_bn (BatchNormal (None, None, None, 9 3968        conv5_block15_concat[0][0]
__________________________________________________________________________________________________
conv5_block16_0_relu (Activatio (None, None, None, 9 0           conv5_block16_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block16_1_conv (Conv2D)   (None, None, None, 1 126976      conv5_block16_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block16_1_bn (BatchNormal (None, None, None, 1 512         conv5_block16_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block16_1_relu (Activatio (None, None, None, 1 0           conv5_block16_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block16_2_conv (Conv2D)   (None, None, None, 3 36864       conv5_block16_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block16_concat (Concatena (None, None, None, 1 0           conv5_block15_concat[0][0]
                                                                 conv5_block16_2_conv[0][0]
__________________________________________________________________________________________________
C5_reduced (Conv2D)             (None, None, None, 2 262400      conv5_block16_concat[0][0]
__________________________________________________________________________________________________
P5_upsampled (UpsampleLike)     (None, None, None, 2 0           C5_reduced[0][0]
                                                                 conv4_block24_concat[0][0]
__________________________________________________________________________________________________
C4_reduced (Conv2D)             (None, None, None, 2 262400      conv4_block24_concat[0][0]
__________________________________________________________________________________________________
P4_merged (Add)                 (None, None, None, 2 0           P5_upsampled[0][0]
                                                                 C4_reduced[0][0]
__________________________________________________________________________________________________
P4_upsampled (UpsampleLike)     (None, None, None, 2 0           P4_merged[0][0]
                                                                 conv3_block12_concat[0][0]
__________________________________________________________________________________________________
C3_reduced (Conv2D)             (None, None, None, 2 131328      conv3_block12_concat[0][0]
__________________________________________________________________________________________________
P6 (Conv2D)                     (None, None, None, 2 2359552     conv5_block16_concat[0][0]
__________________________________________________________________________________________________
P3_merged (Add)                 (None, None, None, 2 0           P4_upsampled[0][0]
                                                                 C3_reduced[0][0]
__________________________________________________________________________________________________
C6_relu (Activation)            (None, None, None, 2 0           P6[0][0]
__________________________________________________________________________________________________
P3 (Conv2D)                     (None, None, None, 2 590080      P3_merged[0][0]
__________________________________________________________________________________________________
P4 (Conv2D)                     (None, None, None, 2 590080      P4_merged[0][0]
__________________________________________________________________________________________________
P5 (Conv2D)                     (None, None, None, 2 590080      C5_reduced[0][0]
__________________________________________________________________________________________________
P7 (Conv2D)                     (None, None, None, 2 590080      C6_relu[0][0]
__________________________________________________________________________________________________
regression_submodel (Model)     (None, None, 4)      2443300     P3[0][0]
                                                                 P4[0][0]
                                                                 P5[0][0]
                                                                 P6[0][0]
                                                                 P7[0][0]
__________________________________________________________________________________________________
classification_submodel (Model) (None, None, 105)    4538545     P3[0][0]
                                                                 P4[0][0]
                                                                 P5[0][0]
                                                                 P6[0][0]
                                                                 P7[0][0]
__________________________________________________________________________________________________
regression (Concatenate)        (None, None, 4)      0           regression_submodel[1][0]
                                                                 regression_submodel[2][0]
                                                                 regression_submodel[3][0]
                                                                 regression_submodel[4][0]
                                                                 regression_submodel[5][0]
__________________________________________________________________________________________________
classification (Concatenate)    (None, None, 105)    0           classification_submodel[1][0]
                                                                 classification_submodel[2][0]
                                                                 classification_submodel[3][0]
                                                                 classification_submodel[4][0]
                                                                 classification_submodel[5][0]
==================================================================================================
Total params: 19,391,253
Trainable params: 19,309,653
Non-trainable params: 81,600
__________________________________________________________________________________________________
None
C:\Programmieren\Anaconda3\lib\site-packages\keras\callbacks.py:999: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.
  warnings.warn('`epsilon` argument is deprecated and '
Epoch 1/500
Strides = [8, 16, 32, 64, 128]
Sizes = [32, 64, 128, 256, 512]
Ratios = [0.5 1.  2. ]
Scales = [1.         1.25992105 1.58740105]
Classifying samples as positive if they overlap with ground truth more than 0.5 and as background if they overlap less than 0.4. Overlaps inbetween will be ignored
2018-07-06 04:50:24.456893: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.73GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 04:50:24.464949: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.86GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 04:50:24.475309: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.99GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 04:50:24.483978: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.12GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 04:50:24.492632: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.25GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 04:50:24.501050: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.38GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 04:50:24.510719: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 04:50:24.519979: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.64GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 04:50:24.529386: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.77GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-07-06 04:50:24.538349: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.90GiB
. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
84/84 [==============================] - 152s 2s/step - loss: 3.7189 - regression_loss: 2.6236 - classification_loss: 1.0953
mAP: 0.0000

Epoch 00001: mAP improved from -inf to 0.00002, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 2/500
84/84 [==============================] - 38s 457ms/step - loss: 3.0959 - regression_loss: 2.3144 - classification_loss: 0.7816
mAP: 0.0000

Epoch 00002: mAP did not improve from 0.00002
Epoch 3/500
84/84 [==============================] - 38s 456ms/step - loss: 2.6954 - regression_loss: 1.9973 - classification_loss: 0.6980
mAP: 0.0001

Epoch 00003: mAP improved from 0.00002 to 0.00010, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 4/500
84/84 [==============================] - 38s 457ms/step - loss: 2.4163 - regression_loss: 1.7625 - classification_loss: 0.6538
mAP: 0.0010

Epoch 00004: mAP improved from 0.00010 to 0.00096, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 5/500
84/84 [==============================] - 38s 456ms/step - loss: 2.1925 - regression_loss: 1.5858 - classification_loss: 0.6067
mAP: 0.0012

Epoch 00005: mAP improved from 0.00096 to 0.00116, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 6/500
84/84 [==============================] - 38s 456ms/step - loss: 1.9915 - regression_loss: 1.4217 - classification_loss: 0.5698
mAP: 0.0005

Epoch 00006: mAP did not improve from 0.00116
Epoch 7/500
84/84 [==============================] - 38s 456ms/step - loss: 1.8162 - regression_loss: 1.2932 - classification_loss: 0.5229
mAP: 0.0014

Epoch 00007: mAP improved from 0.00116 to 0.00140, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 8/500
84/84 [==============================] - 38s 457ms/step - loss: 1.6876 - regression_loss: 1.2042 - classification_loss: 0.4834
mAP: 0.0011

Epoch 00008: mAP did not improve from 0.00140
Epoch 9/500
84/84 [==============================] - 38s 456ms/step - loss: 1.4898 - regression_loss: 1.0548 - classification_loss: 0.4349
mAP: 0.0006

Epoch 00009: mAP did not improve from 0.00140
Epoch 10/500
84/84 [==============================] - 38s 457ms/step - loss: 1.4191 - regression_loss: 1.0053 - classification_loss: 0.4138
mAP: 0.0005

Epoch 00010: mAP did not improve from 0.00140
Epoch 11/500
84/84 [==============================] - 38s 457ms/step - loss: 1.3192 - regression_loss: 0.9396 - classification_loss: 0.3796
mAP: 0.0011

Epoch 00011: mAP did not improve from 0.00140
Epoch 12/500
84/84 [==============================] - 38s 456ms/step - loss: 1.1912 - regression_loss: 0.8507 - classification_loss: 0.3405
mAP: 0.0013

Epoch 00012: mAP did not improve from 0.00140
Epoch 13/500
84/84 [==============================] - 38s 456ms/step - loss: 1.0810 - regression_loss: 0.7706 - classification_loss: 0.3104
mAP: 0.0009

Epoch 00013: mAP did not improve from 0.00140
Epoch 14/500
84/84 [==============================] - 38s 456ms/step - loss: 1.0542 - regression_loss: 0.7588 - classification_loss: 0.2954
mAP: 0.0011

Epoch 00014: mAP did not improve from 0.00140
Epoch 15/500
84/84 [==============================] - 38s 456ms/step - loss: 1.0467 - regression_loss: 0.7585 - classification_loss: 0.2882
mAP: 0.0016

Epoch 00015: mAP improved from 0.00140 to 0.00161, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 16/500
84/84 [==============================] - 38s 456ms/step - loss: 0.9534 - regression_loss: 0.6789 - classification_loss: 0.2745
mAP: 0.0013

Epoch 00016: mAP did not improve from 0.00161
Epoch 17/500
84/84 [==============================] - 38s 456ms/step - loss: 0.9134 - regression_loss: 0.6594 - classification_loss: 0.2541
mAP: 0.0014

Epoch 00017: mAP did not improve from 0.00161
Epoch 18/500
84/84 [==============================] - 38s 457ms/step - loss: 0.9079 - regression_loss: 0.6613 - classification_loss: 0.2466
mAP: 0.0018

Epoch 00018: mAP improved from 0.00161 to 0.00182, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 19/500
84/84 [==============================] - 38s 457ms/step - loss: 0.8094 - regression_loss: 0.5981 - classification_loss: 0.2114
mAP: 0.0015

Epoch 00019: mAP did not improve from 0.00182
Epoch 20/500
84/84 [==============================] - 38s 457ms/step - loss: 0.8070 - regression_loss: 0.5923 - classification_loss: 0.2147
mAP: 0.0013

Epoch 00020: mAP did not improve from 0.00182
Epoch 21/500
84/84 [==============================] - 38s 457ms/step - loss: 0.7617 - regression_loss: 0.5650 - classification_loss: 0.1966
mAP: 0.0019

Epoch 00021: mAP improved from 0.00182 to 0.00185, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 22/500
84/84 [==============================] - 38s 457ms/step - loss: 0.7431 - regression_loss: 0.5557 - classification_loss: 0.1874
mAP: 0.0017

Epoch 00022: mAP did not improve from 0.00185
Epoch 23/500
84/84 [==============================] - 38s 457ms/step - loss: 0.6876 - regression_loss: 0.5142 - classification_loss: 0.1734
mAP: 0.0018

Epoch 00023: mAP did not improve from 0.00185
Epoch 24/500
84/84 [==============================] - 38s 456ms/step - loss: 0.6505 - regression_loss: 0.4859 - classification_loss: 0.1646
mAP: 0.0019

Epoch 00024: mAP improved from 0.00185 to 0.00191, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 25/500
84/84 [==============================] - 38s 456ms/step - loss: 0.6347 - regression_loss: 0.4797 - classification_loss: 0.1550
mAP: 0.0022

Epoch 00025: mAP improved from 0.00191 to 0.00219, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 26/500
84/84 [==============================] - 38s 457ms/step - loss: 0.6318 - regression_loss: 0.4869 - classification_loss: 0.1449
mAP: 0.0027

Epoch 00026: mAP improved from 0.00219 to 0.00271, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 27/500
84/84 [==============================] - 38s 457ms/step - loss: 0.6901 - regression_loss: 0.5181 - classification_loss: 0.1721
mAP: 0.0024

Epoch 00027: mAP did not improve from 0.00271
Epoch 28/500
84/84 [==============================] - 38s 457ms/step - loss: 0.5987 - regression_loss: 0.4656 - classification_loss: 0.1330
mAP: 0.0021

Epoch 00028: mAP did not improve from 0.00271
Epoch 29/500
84/84 [==============================] - 38s 457ms/step - loss: 0.5828 - regression_loss: 0.4593 - classification_loss: 0.1235
mAP: 0.0028

Epoch 00029: mAP improved from 0.00271 to 0.00283, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 30/500
84/84 [==============================] - 38s 456ms/step - loss: 0.5490 - regression_loss: 0.4328 - classification_loss: 0.1161
mAP: 0.0029

Epoch 00030: mAP improved from 0.00283 to 0.00294, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 31/500
84/84 [==============================] - 38s 457ms/step - loss: 0.5285 - regression_loss: 0.4209 - classification_loss: 0.1077
mAP: 0.0021

Epoch 00031: mAP did not improve from 0.00294
Epoch 32/500
84/84 [==============================] - 38s 457ms/step - loss: 0.5555 - regression_loss: 0.4445 - classification_loss: 0.1111
mAP: 0.0033

Epoch 00032: mAP improved from 0.00294 to 0.00335, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 33/500
84/84 [==============================] - 38s 456ms/step - loss: 0.5469 - regression_loss: 0.4375 - classification_loss: 0.1094
mAP: 0.0034

Epoch 00033: mAP improved from 0.00335 to 0.00336, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 34/500
84/84 [==============================] - 38s 457ms/step - loss: 0.5031 - regression_loss: 0.4007 - classification_loss: 0.1024
mAP: 0.0031

Epoch 00034: mAP did not improve from 0.00336
Epoch 35/500
84/84 [==============================] - 38s 456ms/step - loss: 0.4842 - regression_loss: 0.3939 - classification_loss: 0.0903
mAP: 0.0035

Epoch 00035: mAP improved from 0.00336 to 0.00347, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 36/500
84/84 [==============================] - 38s 456ms/step - loss: 0.4719 - regression_loss: 0.3856 - classification_loss: 0.0863
mAP: 0.0043

Epoch 00036: mAP improved from 0.00347 to 0.00433, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 37/500
84/84 [==============================] - 38s 456ms/step - loss: 0.4684 - regression_loss: 0.3858 - classification_loss: 0.0826
mAP: 0.0035

Epoch 00037: mAP did not improve from 0.00433
Epoch 38/500
84/84 [==============================] - 38s 456ms/step - loss: 0.4428 - regression_loss: 0.3621 - classification_loss: 0.0807
mAP: 0.0027

Epoch 00038: mAP did not improve from 0.00433
Epoch 39/500
84/84 [==============================] - 38s 456ms/step - loss: 0.4324 - regression_loss: 0.3587 - classification_loss: 0.0737
mAP: 0.0038

Epoch 00039: mAP did not improve from 0.00433
Epoch 40/500
84/84 [==============================] - 38s 456ms/step - loss: 0.4259 - regression_loss: 0.3564 - classification_loss: 0.0695
mAP: 0.0025

Epoch 00040: mAP did not improve from 0.00433
Epoch 41/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3903 - regression_loss: 0.3235 - classification_loss: 0.0668
mAP: 0.0035

Epoch 00041: mAP did not improve from 0.00433
Epoch 42/500
84/84 [==============================] - 38s 456ms/step - loss: 0.4191 - regression_loss: 0.3503 - classification_loss: 0.0688
mAP: 0.0026

Epoch 00042: mAP did not improve from 0.00433
Epoch 43/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3892 - regression_loss: 0.3248 - classification_loss: 0.0644
mAP: 0.0040

Epoch 00043: mAP did not improve from 0.00433
Epoch 44/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3763 - regression_loss: 0.3146 - classification_loss: 0.0617
mAP: 0.0042

Epoch 00044: mAP did not improve from 0.00433
Epoch 45/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3883 - regression_loss: 0.3318 - classification_loss: 0.0565
mAP: 0.0055

Epoch 00045: mAP improved from 0.00433 to 0.00547, saving model to ./snapshots\densenet121_mob_csv_2018-07-06_04-49.h5
Epoch 46/500
84/84 [==============================] - 38s 457ms/step - loss: 0.3501 - regression_loss: 0.2989 - classification_loss: 0.0512
mAP: 0.0026

Epoch 00046: mAP did not improve from 0.00547
Epoch 47/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3637 - regression_loss: 0.3015 - classification_loss: 0.0622
mAP: 0.0033

Epoch 00047: mAP did not improve from 0.00547
Epoch 48/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3355 - regression_loss: 0.2872 - classification_loss: 0.0483
mAP: 0.0021

Epoch 00048: mAP did not improve from 0.00547
Epoch 49/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3335 - regression_loss: 0.2904 - classification_loss: 0.0431
mAP: 0.0050

Epoch 00049: mAP did not improve from 0.00547
Epoch 50/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3329 - regression_loss: 0.2908 - classification_loss: 0.0421
mAP: 0.0038

Epoch 00050: mAP did not improve from 0.00547
Epoch 51/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3019 - regression_loss: 0.2649 - classification_loss: 0.0371
mAP: 0.0039

Epoch 00051: mAP did not improve from 0.00547
Epoch 52/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3212 - regression_loss: 0.2842 - classification_loss: 0.0371
mAP: 0.0037

Epoch 00052: mAP did not improve from 0.00547
Epoch 53/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3201 - regression_loss: 0.2863 - classification_loss: 0.0338
mAP: 0.0038

Epoch 00053: mAP did not improve from 0.00547
Epoch 54/500
84/84 [==============================] - 38s 456ms/step - loss: 0.2814 - regression_loss: 0.2489 - classification_loss: 0.0326
mAP: 0.0043

Epoch 00054: mAP did not improve from 0.00547
Epoch 55/500
84/84 [==============================] - 38s 456ms/step - loss: 0.3115 - regression_loss: 0.2610 - classification_loss: 0.0505
mAP: 0.0052

Epoch 00055: mAP did not improve from 0.00547
Epoch 56/500
84/84 [==============================] - 38s 457ms/step - loss: 0.3062 - regression_loss: 0.2683 - classification_loss: 0.0379
mAP: 0.0029

Epoch 00056: mAP did not improve from 0.00547
Epoch 57/500
84/84 [==============================] - 38s 456ms/step - loss: 0.2840 - regression_loss: 0.2515 - classification_loss: 0.0325
mAP: 0.0042

Epoch 00057: mAP did not improve from 0.00547
Epoch 58/500
84/84 [==============================] - 38s 456ms/step - loss: 0.2732 - regression_loss: 0.2457 - classification_loss: 0.0275
mAP: 0.0037

Epoch 00058: mAP did not improve from 0.00547
Epoch 59/500
84/84 [==============================] - 38s 457ms/step - loss: 0.2602 - regression_loss: 0.2351 - classification_loss: 0.0251
mAP: 0.0047

Epoch 00059: mAP did not improve from 0.00547
Epoch 60/500
84/84 [==============================] - 38s 456ms/step - loss: 0.2617 - regression_loss: 0.2363 - classification_loss: 0.0254
mAP: 0.0040

Epoch 00060: mAP did not improve from 0.00547
Epoch 61/500
84/84 [==============================] - 38s 457ms/step - loss: 0.2624 - regression_loss: 0.2358 - classification_loss: 0.0266
mAP: 0.0045

Epoch 00061: mAP did not improve from 0.00547

Epoch 00061: ReduceLROnPlateau reducing learning rate to 7.999999797903001e-05.
Epoch 62/500
84/84 [==============================] - 38s 457ms/step - loss: 0.2235 - regression_loss: 0.2046 - classification_loss: 0.0189
mAP: 0.0044

Epoch 00062: mAP did not improve from 0.00547
Epoch 63/500
84/84 [==============================] - 38s 456ms/step - loss: 0.1948 - regression_loss: 0.1806 - classification_loss: 0.0142
mAP: 0.0040

Epoch 00063: mAP did not improve from 0.00547
Epoch 64/500
84/84 [==============================] - 38s 456ms/step - loss: 0.2046 - regression_loss: 0.1901 - classification_loss: 0.0145
mAP: 0.0030

Epoch 00064: mAP did not improve from 0.00547
Epoch 65/500
84/84 [==============================] - 38s 456ms/step - loss: 0.1946 - regression_loss: 0.1824 - classification_loss: 0.0122
mAP: 0.0027

Epoch 00065: mAP did not improve from 0.00547
Epoch 66/500
84/84 [==============================] - 38s 456ms/step - loss: 0.1942 - regression_loss: 0.1821 - classification_loss: 0.0121
mAP: 0.0043

Epoch 00066: mAP did not improve from 0.00547
Epoch 67/500
84/84 [==============================] - 38s 456ms/step - loss: 0.1863 - regression_loss: 0.1734 - classification_loss: 0.0129
mAP: 0.0037

Epoch 00067: mAP did not improve from 0.00547
Epoch 68/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1846 - regression_loss: 0.1745 - classification_loss: 0.0101
mAP: 0.0037

Epoch 00068: mAP did not improve from 0.00547
Epoch 69/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1916 - regression_loss: 0.1815 - classification_loss: 0.0101
mAP: 0.0034

Epoch 00069: mAP did not improve from 0.00547
Epoch 70/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1948 - regression_loss: 0.1841 - classification_loss: 0.0107
mAP: 0.0046

Epoch 00070: mAP did not improve from 0.00547
Epoch 71/500
84/84 [==============================] - 38s 456ms/step - loss: 0.1862 - regression_loss: 0.1740 - classification_loss: 0.0122
mAP: 0.0038

Epoch 00071: mAP did not improve from 0.00547
Epoch 72/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1720 - regression_loss: 0.1624 - classification_loss: 0.0096
mAP: 0.0035

Epoch 00072: mAP did not improve from 0.00547
Epoch 73/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1747 - regression_loss: 0.1658 - classification_loss: 0.0089
mAP: 0.0040

Epoch 00073: mAP did not improve from 0.00547
Epoch 74/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1754 - regression_loss: 0.1670 - classification_loss: 0.0085
mAP: 0.0048

Epoch 00074: mAP did not improve from 0.00547
Epoch 75/500
84/84 [==============================] - 38s 456ms/step - loss: 0.1714 - regression_loss: 0.1638 - classification_loss: 0.0076
mAP: 0.0038

Epoch 00075: mAP did not improve from 0.00547
Epoch 76/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1683 - regression_loss: 0.1619 - classification_loss: 0.0065
mAP: 0.0038

Epoch 00076: mAP did not improve from 0.00547
Epoch 77/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1665 - regression_loss: 0.1587 - classification_loss: 0.0078
mAP: 0.0034

Epoch 00077: mAP did not improve from 0.00547

Epoch 00077: ReduceLROnPlateau reducing learning rate to 6.399999838322402e-05.
Epoch 78/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1446 - regression_loss: 0.1385 - classification_loss: 0.0061
mAP: 0.0048

Epoch 00078: mAP did not improve from 0.00547
Epoch 79/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1262 - regression_loss: 0.1221 - classification_loss: 0.0041
mAP: 0.0041

Epoch 00079: mAP did not improve from 0.00547
Epoch 80/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1193 - regression_loss: 0.1154 - classification_loss: 0.0039
mAP: 0.0035

Epoch 00080: mAP did not improve from 0.00547
Epoch 81/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1157 - regression_loss: 0.1122 - classification_loss: 0.0036
mAP: 0.0038

Epoch 00081: mAP did not improve from 0.00547
Epoch 82/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1271 - regression_loss: 0.1231 - classification_loss: 0.0040
mAP: 0.0035

Epoch 00082: mAP did not improve from 0.00547
Epoch 83/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1222 - regression_loss: 0.1179 - classification_loss: 0.0042
mAP: 0.0041

Epoch 00083: mAP did not improve from 0.00547
Epoch 84/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1224 - regression_loss: 0.1188 - classification_loss: 0.0037
mAP: 0.0040

Epoch 00084: mAP did not improve from 0.00547
Epoch 85/500
84/84 [==============================] - 38s 457ms/step - loss: 0.1259 - regression_loss: 0.1223 - classification_loss: 0.0036
mAP: 0.0038

Epoch 00085: mAP did not improve from 0.00547
Epoch 00085: early stopping
Trained for: 1:08:22.269863
**********************
Windows PowerShell transcript end
End time: 20180706055800
**********************
